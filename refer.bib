@article{scheibler_blinkies_2020,
  author  = {Scheibler, Robin and Ono, Nobutaka},
  title   = {{Blinkies: {Open} {Source} {Sound}-to-{Light} {Conversion} {Sensors} for {Large}-{Scale} {Acoustic} {Sensing} and {Applications}}},
  journal = {IEEE Access},
  year    = {2020},
  volume  = {8},
  pages   = {67603--67616},
  doi     = {10.1109/ACCESS.2020.2985281}
}

@misc{srivastava2023omniveclearningrobustrepresentations,
  author        = {Siddharth Srivastava and Gaurav Sharma},
  title         = {{OmniVec: Learning robust representations with cross modal sharing}},
  year          = {2023},
  eprint        = {2311.05709},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@misc{tokozume2018betweenclasslearningimageclassification,
  author        = {Yuji Tokozume and Yoshitaka Ushiku and Tatsuya Harada},
  title         = {{Between-class Learning for Image Classification}},
  year          = {2018},
  eprint        = {1711.10284},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{kingma_adam_2017,
  author        = {Diederik P. Kingma and Jimmy Ba},
  title         = {{Adam: A Method for Stochastic Optimization}},
  year          = {2017},
  eprint        = {1412.6980},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@inproceedings{Scheibler_pyroom_2018,
  author    = {Scheibler, Robin and Bezzam, Eric and Dokmanic, Ivan},
  title     = {{Pyroomacoustics: A Python Package for Audio Room Simulation and Array Processing Algorithms}},
  booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2018},
  pages     = {351--355},
  publisher = {IEEE},
  doi       = {10.1109/icassp.2018.8461310}
}
@inproceedings{nishida_estimation_2022,
  address   = {Chiang Mai, Thailand},
  title     = {Estimation of {Transfer} {Coefficients} and {Signals} of {Sound}-to-{Light} {Conversion} {Device} {Blinky} {Under} {Saturation}},
  isbn      = {978-616-590-477-3},
  doi       = {10.23919/APSIPAASC55919.2022.9980090},
  language  = {en},
  urldate   = {2023-11-15},
  booktitle = {2022 {Asia}-{Pacific} {Signal} and {Information} {Processing} {Association} {Annual} {Summit} and {Conference} ({APSIPA} {ASC})},
  publisher = {IEEE},
  author    = {Nishida, Kosuke and Ueno, Natsuki and Kinoshita, Yuma and Ono, Nobutaka},
  month     = nov,
  year      = {2022},
  pages     = {717--722}
}


@article{Mohaimenuzzaman_2023_acdnet,
  author    = {Mohaimenuzzaman, Md and Bergmeir, Christoph and West, Ian and Meyer, Bernd},
  title     = {{Environmental Sound Classiﬁcation on the Edge: A Pipeline for Deep Acoustic Networks on Extremely Resource-Constrained Devices}},
  journal   = {Pattern Recognition},
  year      = {2023},
  volume    = {133},
  pages     = {109025},
  doi       = {10.1016/j.patcog.2022.109025},
  publisher = {Elsevier BV}
}

@article{stowell_detection_2015,
  author  = {Stowell, Dan and Giannoulis, Dimitrios and Benetos, Emmanouil and Lagrange, Mathieu and Plumbley, Mark D.},
  title   = {{Detection and {Classification} of {Acoustic} {Scenes} and {Events}}},
  journal = {IEEE Transactions on Multimedia},
  year    = {2015},
  volume  = {17},
  number  = {10},
  pages   = {1733--1746},
  doi     = {10.1109/TMM.2015.2428998}
}

@misc{zhang2024soundeventlocalizationclassification,
  author        = {Dongzhe Zhang and Jianfeng Chen and Jisheng Bai and Mou Wang},
  title         = {{Sound event localization and classification using WASN in Outdoor Environment}},
  year          = {2024},
  eprint        = {2403.20130},
  archiveprefix = {arXiv},
  primaryclass  = {cs.SD}
}

@misc{huang2024tinychirpbirdsongrecognition,
  author        = {Zhaolan Huang and Adrien Tousnakhoff and Polina Kozyr and Roman Rehausen and Felix Bießmann and Robert Lachlan and Cedric Adjih and Emmanuel Baccelli},
  title         = {{TinyChirp: Bird Song Recognition Using TinyML Models on Low-power Wireless Acoustic Sensors}},
  year          = {2024},
  eprint        = {2407.21453},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@article{cherkassky_blind_2017,
  author   = {Cherkassky, Dani and Gannot, Sharon},
  journal  = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  title    = {Blind Synchronization in Wireless Acoustic Sensor Networks},
  year     = {2017},
  volume   = {25},
  number   = {3},
  pages    = {651-661},
  keywords = {Microphones;Correlation;Wideband;Synchronization;Estimation;Array signal processing;Speech;Blind synchronization;distributed microphone array;sampling rate offset;wireless acoustic sensor network;wideband correlation processing},
  doi      = {10.1109/TASLP.2017.2655259}
}

@misc{kong2020pannslargescalepretrainedaudio,
  author        = {Qiuqiang Kong and Yin Cao and Turab Iqbal and Yuxuan Wang and Wenwu Wang and Mark D. Plumbley},
  title         = {{PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition}},
  year          = {2020},
  eprint        = {1912.10211},
  archiveprefix = {arXiv},
  primaryclass  = {cs.SD}
}

@misc{chen2022htsathierarchicaltokensemanticaudio,
  author        = {Ke Chen and Xingjian Du and Bilei Zhu and Zejun Ma and Taylor Berg-Kirkpatrick and Shlomo Dubnov},
  title         = {{HTS-AT: A Hierarchical Token-Semantic Audio Transformer for Sound Classification and Detection}},
  year          = {2022},
  eprint        = {2202.00874},
  archiveprefix = {arXiv},
  primaryclass  = {cs.SD}
}
@article{kinoshita_e2e_2024,
  author     = {Kinoshita, Yuma and Ono, Nobutaka},
  title      = {End-to-end training of acoustic scene classification using distributed sound-to-light conversion devices: verification through simulation experiments},
  year       = {2024},
  issue_date = {Dec 2024},
  publisher  = {Hindawi Limited},
  address    = {London, GBR},
  volume     = {2024},
  number     = {1},
  issn       = {1687-4714},
  url        = {https://doi.org/10.1186/s13636-024-00369-z},
  doi        = {10.1186/s13636-024-00369-z},
  abstract   = {We propose a framework for classifying acoustic scenes utilizing distributed sound sensor devices capable of sound-to-light conversion, which we term as Blinkies. These Blinkies can convert acoustic signals into varying intensities of light via an inbuilt light-emitting diode. By using Blinkies, we can aggregate the spatial acoustic information across a wide region by recording the fluctuating light intensities of numerous Blinkies distributed throughout the region. Nonetheless, the signal communicated is subject to the bandwidth limitation imposed by the frame rate of the video camera, typically capped at 30 frames per second. Our objective is to refine the process of transforming sound into light for the purpose of acoustic scene classification within these bandwidth confines. While traversing through the air, a light signal is affected by inherent physical limitations such as the attenuation of light and interference from noise. To account for these factors, we have integrated these physical constraints into differentiable physical layers. This approach enables us to jointly train a pair of deep neural networks for the conversion of sound to light and for the classification of acoustic scenes. Our simulation studies, which employed the SINS database for acoustic scene classification, demonstrated that our proposed framework outperforms the previous one that utilized Blinkies. These findings emphasize the effectiveness of Blinkies in the field of acoustic scene classification.},
  journal    = {EURASIP J. Audio Speech Music Process.},
  month      = sep,
  numpages   = {14},
  keywords   = {Blinky, Sound-to-light conversion, Deep learning, Differentiable physical layer, Sensor network}
}
@inproceedings{kinoshita2021analysis,
  author    = {Yuma Kinoshita and Nobutaka Ono},
  title     = {Analysis on Roles of DNNs in End-to-End Acoustic Scene Analysis Framework with Distributed Sound-to-Light Conversion Devices},
  booktitle = {APSIPA Annual Summit and Conference},
  pages     = {1167--1172},
  address   = {Tokyo, Japan},
  month     = dec,
  year      = {2021}
}


@inproceedings{kinoshita_end--end_2021,
  author    = {Kinoshita, Yuma and Ono, Nobutaka},
  title     = {{End-to-{End} {Training} for {Acoustic} {Scene} {Analysis} with {Distributed} {Sound}-to-{Light} {Conversion} {Devices}}},
  booktitle = {2021 29th {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
  year      = {2021},
  pages     = {1010--1014},
  publisher = {IEEE},
  doi       = {10.23919/EUSIPCO54536.2021.9616341}
}

@inproceedings{gemmeke_audio_2017,
  author    = {Gemmeke, Jort F. and Ellis, Daniel P. W. and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R. Channing and Plakal, Manoj and Ritter, Marvin},
  title     = {{Audio {Set}: {An} ontology and human-labeled dataset for audio events}},
  booktitle = {2017 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
  year      = {2017},
  pages     = {776--780},
  publisher = {IEEE},
  doi       = {10.1109/ICASSP.2017.7952261}
}

@misc{he_deep_2015,
  author        = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  title         = {{Deep {Residual} {Learning} for {Image} {Recognition}}},
  year          = {2015},
  eprint        = {1512.03385},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@article{ishii_real-time_2021,
  author  = {Ishii, Kanato and Kinoshita, Yuma and Wakabayashi, Yukoh and Ono, Nobutaka},
  title   = {{Real-{Time} {Pitch} {Visualization} with “{Blinky}" {Sound}-to-{Light} {Conversion} {Device}}},
  journal = {Journal of Signal Processing},
  year    = {2021},
  volume  = {25},
  number  = {6},
  pages   = {213--220},
  doi     = {10.2299/jsp.25.213}
}

@inproceedings{horiike_blink-former_2019,
  author    = {Horiike, Daiki and Scheibler, Robin and Wakabayashi, Yukoh and Ono, Nobutaka},
  title     = {{Blink-former: {Light}-aided beamforming for multiple targets enhancement}},
  booktitle = {2019 {IEEE} 21st {International} {Workshop} on {Multimedia} {Signal} {Processing} ({MMSP})},
  year      = {2019},
  pages     = {1--6},
  publisher = {IEEE},
  doi       = {10.1109/MMSP.2019.8901799}
}

@inproceedings{scheibler_multi-modal_2019,
  author    = {Scheibler, Robin and Ono, Nobutaka},
  title     = {{Multi-modal {Blind} {Source} {Separation} with {Microphones} and {Blinkies}}},
  booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
  year      = {2019},
  pages     = {366--370},
  publisher = {IEEE},
  doi       = {10.1109/ICASSP.2019.8682594}
}

@misc{tokozume_bclearning_2018,
  author        = {Yuji Tokozume and Yoshitaka Ushiku and Tatsuya Harada},
  title         = {{Learning from Between-class Examples for Deep Sound Recognition}},
  year          = {2018},
  eprint        = {1711.10282},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{he_2015_deepresiduallearningimage,
  author        = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  title         = {{Deep Residual Learning for Image Recognition}},
  year          = {2015},
  eprint        = {1512.03385},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@misc{paszke_2019_pytorch,
  author        = {Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas Köpf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
  title         = {{PyTorch: An Imperative Style, High-Performance Deep Learning Library}},
  year          = {2019},
  eprint        = {1912.01703},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@inproceedings{scheibler_2018_Pyroomacoustics,
  author    = {Scheibler, Robin and Bezzam, Eric and Dokmanic, Ivan},
  title     = {{Pyroomacoustics: A Python Package for Audio Room Simulation and Array Processing Algorithms}},
  booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2018},
  pages     = {351--355},
  publisher = {IEEE},
  doi       = {10.1109/icassp.2018.8461310}
}

@inproceedings{scheibler_blinkies_2018,
  author    = {Scheibler, Robin and Horiike, Daiki and Ono, Nobutaka},
  title     = {{Blinkies: {Sound}-to-light conversion sensors and their application to speech enhancement and sound source localization}},
  booktitle = {2018 {Asia}-{Pacific} {Signal} and {Information} {Processing} {Association} {Annual} {Summit} and {Conference} ({APSIPA} {ASC})},
  year      = {2018},
  pages     = {1899--1904},
  publisher = {IEEE},
  doi       = {10.23919/APSIPA.2018.8659793}
}

@article{imoto_introduction_2018,
  author  = {Imoto, Keisuke},
  title   = {{Introduction to acoustic event and scene analysis}},
  journal = {Acoustical Science and Technology},
  year    = {2018},
  volume  = {39},
  number  = {3},
  pages   = {182--188},
  doi     = {10.1250/ast.39.182}
}

@inproceedings{esc50_2015,
  author    = {Piczak, Karol J.},
  title     = {{ESC}: {Dataset} for {Environmental Sound Classification}},
  booktitle = {Proceedings of the 23rd {Annual ACM Conference} on {Multimedia}},
  year      = {2015},
  pages     = {1015--1018},
  publisher = {{ACM Press}},
  doi       = {10.1145/2733373.2806390}
}

@misc{gong2021astaudiospectrogramtransformer,
  title         = {AST: Audio Spectrogram Transformer},
  author        = {Yuan Gong and Yu-An Chung and James Glass},
  year          = {2021},
  eprint        = {2104.01778},
  archiveprefix = {arXiv},
  primaryclass  = {cs.SD}
}

@misc{wu2024largescalecontrastivelanguageaudiopretraining,
  title         = {Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation},
  author        = {Yusong Wu and Ke Chen and Tianyu Zhang and Yuchen Hui and Marianna Nezhurina and Taylor Berg-Kirkpatrick and Shlomo Dubnov},
  year          = {2024},
  eprint        = {2211.06687},
  archiveprefix = {arXiv},
  primaryclass  = {cs.SD},
  url           = {https://arxiv.org/abs/2211.06687}
}

@misc{radford2021learningtransferablevisualmodels,
  title         = {Learning Transferable Visual Models From Natural Language Supervision},
  author        = {Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  year          = {2021},
  eprint        = {2103.00020},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2103.00020}
}

@misc{chen2020simpleframeworkcontrastivelearning,
  title         = {A Simple Framework for Contrastive Learning of Visual Representations},
  author        = {Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey Hinton},
  year          = {2020},
  eprint        = {2002.05709},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2002.05709}
}

@misc{he2020momentumcontrastunsupervisedvisual,
  title         = {Momentum Contrast for Unsupervised Visual Representation Learning},
  author        = {Kaiming He and Haoqi Fan and Yuxin Wu and Saining Xie and Ross Girshick},
  year          = {2020},
  eprint        = {1911.05722},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1911.05722}
}

@misc{khosla2021supervisedcontrastivelearning,
  title         = {Supervised Contrastive Learning},
  author        = {Prannay Khosla and Piotr Teterwak and Chen Wang and Aaron Sarna and Yonglong Tian and Phillip Isola and Aaron Maschinot and Ce Liu and Dilip Krishnan},
  year          = {2021},
  eprint        = {2004.11362},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2004.11362}
}

@article{schonemann_1966,
  title   = {A Generalized Solution of the Orthogonal Procrustes Problem},
  volume  = {31},
  doi     = {10.1007/BF02289451},
  number  = {1},
  journal = {Psychometrika},
  author  = {Schönemann, Peter H.},
  year    = {1966},
  pages   = {1–10}
} <div></div>