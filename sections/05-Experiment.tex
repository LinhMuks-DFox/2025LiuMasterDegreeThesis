\section{Experiments}
\label{sec:experiments}

We evaluated the efficacy of our AE–based sound-to-light conversion for AEC through a simulation study.
The experimental design rests on a straightforward premise: embedding functions that generate more discriminative features should yield latent representations $\boldsymbol{z}$ that result in a superior classification performance (e.g.\, macro F1-score).
\subsection{Experimental Setup}
\label{subsec:exp_setup}
The experiments were conducted in a simulated rectangular room measuring $8 \times 6 \times 4\,\text{m}^{3}$, where five Blinkies were deployed.
The process of estimating the class label $\hat{y}$ from the waveform $\boldsymbol{x}_i$ received by the $i$-th Blinky follows the procedure
outlined in Eqs.~\ref{eq:ps_embedding} to \ref{eq:ps_classify}.
AEC performance was measured using the $F_{1}$ score
and model selection relied on the highest $F_{1}$ score obtained on a validation set.

To assess the contribution of our AE-based sound-to-light conversion,
we compared the following configurations for $\mathrm{Embed}(\boldsymbol{x}_i)$:
\begin{itemize}
      \item \textbf{Log-Mel spectrogram without embedding}:
            Converts the raw waveform $\boldsymbol{x}$ into a log-Mel spectrogram $\boldsymbol{X}$, with no further embedding, i.e., $\mathrm{Embed}(\boldsymbol{x})=\boldsymbol{X}$. This evaluation is provided as a reference for the ideal classification performance on the same data, without the Blinky channel constraints.
      \item \textbf{Sound Power}:
            Computes $\boldsymbol{z}$ as the signal power, i.e., $\mathrm{Embed}(\boldsymbol{x})=(x(1)^2, \cdots, x(T)^2)^\top$, following the approach in \cite{scheibler_blinkies_2020}.
      \item \textbf{End-to-End}:
            Computes $\boldsymbol{z}$ by using a DNN encoder $E_{\phi}(\cdot)$, i.e., $\mathrm{Embed}(\boldsymbol{x}) = E_{\phi}(\boldsymbol{X})$. In this method, both the encoder and classifier are jointly optimized in an end-to-end fashion
            using the classification loss.
            The encoder architecture is identical to that of our pre-trained encoder $E_{\phi}$.
      \item \textbf{Autoencoder}:
            Computes $\boldsymbol{z}$ as the latent representation produced by an autoencoder pre-trained with Eq. \ref{eq:recon_loss}, i.e.,
            $\mathrm{Embed}(\boldsymbol{x})=E_{{\phi}^{*}}(\mathrm{Logmel}(\boldsymbol{x}))$.
      \item \textbf{Noise-robust autoencoder (Ours)}:
            Computes $\boldsymbol{z}$ as the latent representation from our noise-aware autoencoder pre-trained with Eq.~\ref{eq:recon_loss_robust},
            i.e., $\mathrm{Embed}(\boldsymbol{x})=E_{{\phi}^{*}}(\mathrm{Logmel}(\boldsymbol{x}))$.
\end{itemize}

We utilized the ESC-50 dataset~\cite{esc50_2015}, partitioned into training, validation, and test sets with an 8:1:1 ratio
on a per-class basis to ensure class-wise balance across all subsets.
The acoustic waveform $\boldsymbol{x}_i$ received at the $i$-th Blinky was generated by convolving a source signal $s$ with the corresponding room impulse response (RIR) between that source and the Blinky.
To simulate class-dependent spatial characteristics,
a single sound source was randomly positioned for each class, with the $k$-th source assigned to the $k$-th class.
Source and Blinky positions remained fixed throughout the experiment.
RIRs were generated using the image-source method implemented in the \textit{pyroomacoustics}~\cite{Scheibler_pyroom_2018},
with a wall-absorption coefficient of 0.4 and a maximum image source order of 10, then zero-padded to equal length.
Parameter $a$, $b$, and $\sigma$ of Eq.~\ref{eq:ps_distort} were $1$, $0.1$, and $0.05$, respectively.

The classifier $\mathrm{Classify}(\cdot)$ was implemented as a ResNet-18~\cite{he_deep_2015}.
During training the classifier,
we applied BC-learning~\cite{tokozume_bclearning_2018}
as a data augmentation,
wherein two samples from different classes and their one-hot labels were linearly combined to produce mixed inputs and soft targets.
The classifier was trained using the Kullback–Leibler (KL) divergence
as the loss function and Adam optimizer~\cite{kingma_adam_2017}.
As in the pre-training phase, we used the \texttt{ReduceLROnPlateau} scheduler in the classifier training phase, initializing the learning rate of $1 \times 10^{-2}$. The scheduler monitored the validation loss and reduced the learning rate by a factor of 0.5 whenever the validation loss plateaued.

This simulation experiment was implemented using the \textit{PyTorch} framework~\cite{paszke_2019_pytorch}.

\subsection{Results}
\label{subsec:results}
\input{sections/tables/result.tex}
Table~\ref{tab:exp_results_f1} presents macro-averaged $F_{1}$ scores on the test set under three transmission scenarios: (i) no degradation, $\mathrm{Transmit}(\boldsymbol{z})=\boldsymbol{z}$; (ii) resampling only, $\mathrm{Transmit}(\boldsymbol{z})=\mathrm{Resample}(\boldsymbol{z})$; and (iii) resampling followed by optical distortions, $\mathrm{Transmit}(\boldsymbol{z})=(\mathrm{Resample}\circ\mathrm{Distort})(\boldsymbol{z})$.
Except for the log-Mel spectrogram without embedding, each method was evaluated four times with different random seeds, and the mean and standard deviation across these four runs are reported in Table~\ref{tab:exp_results_f1}.

Across all conditions, our AE-based method substantially outperforms both the sound-power and the end-to-end approaches.
Moreover, our noise-robust autoencoder achieves higher $F_{1}$ scores than the standard autoencoder,
highlighting the effectiveness of the noise-aware training objective described in Eq.~\ref{eq:recon_loss_robust}.

\subsection{Discussion and Limitations}
Through simulation experiments, we demonstrated the efficacy of the proposed noise-robust autoencoder for sound-to-light conversion. Nevertheless, because these experiments were purely virtual, its performance under real-world conditions remains unverified.
Our immediate objective is therefore to implement the framework in a physical environment and conduct a comprehensive evaluation of its AEC accuracy.
This assessment will also quantify the computational overhead and determine the feasibility of real-time processing on embedded hardware.

Moreover, the present study does not incorporate the spatial characteristics inherent in audio signals during AE training. Consequently, the latent vectors may lack room-impulse response (RIR) information contained in the original signal,
potentially diminishing the spatial cues available for AEC.
Designing a loss function that explicitly preserves spatial information could yield richer embeddings and further improve performance.

Another limitation of this study is that our evaluation focuses exclusively on single-event clips and does not address scenarios in which two or more acoustic events occur simultaneously. This work primarily emphasizes validating whether the proposed method functions as intended, rather than exhaustively delineating its operational boundaries; therefore, multi-event classification is deferred to future investigations. Nevertheless, since the downstream classifier is trained with BC learning—thereby being exposed to mixtures and soft labels—the resulting decision function is capable of handling mixed inputs to some extent, indicating a promising potential for the recognition of concurrent events in more complex environments.

It is further assumed that each Blinky transforms its recorded audio into a low-dimensional vector within a shared 5-second time window, and it is not assumed that recordings occur over differing temporal intervals across devices. However, this assumption may not hold in practical deployments, where device-specific asynchrony can arise. Consequently, the design of robust synchronization mechanisms and the development of training or inference strategies resilient to temporal misalignment represent important avenues for future research.
Finally, although we employed an autoencoder for unsupervised representation learning, alternative methods---such as generative approaches (e.g., variational autoencoders) and contrastive learning objectives---warrant investigation.