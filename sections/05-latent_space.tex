\section{Visualization of Latent Representations}
\label{sec:analysis}

While in~\ref{sec:experiments} demonstrates the superior classification performance of the proposed method under limited bandwidth, the internal mechanism of the autoencoder remains a "black box." Specifically, it is unclear what features the encoder prioritizes when compressing audio into such a compact latent space (15 Hz) without semantic supervision. This section investigates the intrinsic structure of the latent space through visualization and quantitative acoustic verification.

\subsection{Global Latent Geometry Visualization}
\label{subsec:visualization}

To intuitively understand the topological structure of the learned representations, we visualized the latent spaces generated by both the normal and noise-robust encoders. We performed inference on the complete ESC-50 dataset ($N=2000$) to extract high-dimensional latent variables, resulting in a data matrix $\mathbf{Z} \in \mathbb{R}^{2000 \times 300}$.

Dimensionality reduction techniques, specifically Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE), were employed to project the 300-dimensional latent vectors onto a 2D plane. The visualization results are presented in Fig.~\ref{fig:pca_noise}, Fig.~\ref{fig:pca_normal}, Fig.~\ref{fig:tsne_noise} and Fig.~\ref{fig:tsne_normal}.

\input{sections/pdf-png-ref/pca_noised.tex}
\input{sections/pdf-png-ref/pca_normal.tex}
\input{sections/pdf-png-ref/tsne_noised.tex}
\input{sections/pdf-png-ref/tsne_normal.tex}



\subsection{Symmetry and Structural Isomorphism}
\label{subsec:symmetry}

A notable observation from the low-dimensional projections (both PCA and t-SNE) is the apparent symmetry between the manifolds of the normal and noise-robust encoders. Although dimensionality reduction algorithms often possess inherent sign ambiguities (e.g., the arbitrary sign of eigenvectors in PCA), the consistent symmetric patterns motivated us to investigate whether this geometric relationship holds in the native 300-dimensional space.

To quantitatively verify this hypothesis, we conducted a geometric alignment analysis between the latent variables produced by the normal encoder ($\mathbf{Z}_{norm}$) and the noise-robust encoder ($\mathbf{Z}_{rob}$).

First, we calculated the sample-peer cosine similarity between the paired latent vectors. The average cosine similarity reached \textbf{0.97}, indicating that the two encoders, despite being trained with different objectives, have learned highly similar semantic manifolds.

Furthermore, to explicitly determine the geometric transformation between the two spaces, we formulated the alignment as an Orthogonal Procrustes\cite{schonemann_1966} problem. We sought an orthogonal matrix $\mathbf{Q}$ that aligns $\mathbf{Z}_{norm}$ to $\mathbf{Z}_{rob}$ by minimizing the Frobenius norm:
\begin{equation}
    \min_{\mathbf{Q} \in \mathbb{R}^{300 \times 300}} ||\mathbf{Z}_{rob}\mathbf{Q} - \mathbf{Z}_{norm}||_F, \quad \text{s.t. } \mathbf{Q}^T\mathbf{Q} = \mathbf{I}
\end{equation}
The determinant of the optimal rotation matrix was calculated to be $\det(\mathbf{Q}) \approx 1$.

\subsection{Discussion}
This result ($\det(\mathbf{Q}) \approx 1$) implied that $\mathbf{Z}_{norm}$ and $\mathbf{Z}_{rob}$ can be aligned almost perfectly through a simple proper rotation. This suggests that the injection of Gaussian noise into the latent variables during training does not fundamentally alter the topology of the learned manifold. The average sample-wise cosine similarity between paired samples, however, is not exactly $1$, and the residual reconstruction error (Frobenius norm) is non-zero (about 0.23), which we interpret as small but systematic geometric deviations between the two manifolds.

Given the superior classification performance demonstrated in Section~\ref{sec:experiments}, we \textbf{hypothesize} that while the global geometric structure remains largely invariant, these minute deviations represent a refinement of the feature space. It is precisely these subtle structural adjustments that equip the Autoencoder with its observed resilience to channel noise.